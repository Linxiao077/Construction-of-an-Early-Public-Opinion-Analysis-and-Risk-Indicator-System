#è¿™æ˜¯å¤„ç†æ•°æ®çš„ç¬¬äºŒä¸ªæ–‡ä»¶
#å®šä¹‰åª’ä½“æƒå¨åº¦æ˜ å°„å­—å…¸ï¼Œåˆ†ä¸º7ä¸ªæƒå¨åº¦ã€‚åˆ©ç”¨hammingè·ç¦»æ‰¾å‡ºç›¸ä¼¼æ–°é—»ï¼Œæ ¹æ®ç›¸å…³ç­–ç•¥ï¼Œä¿ç•™æƒå¨åº¦é«˜ï¼Œå‘å¸ƒæ—¶é—´æ—©çš„æ–°é—»ã€‚æœ€åæ‰¹é‡å¤„ç†ã€‚

import pandas as pd
import numpy as np
from simhash import Simhash
from datetime import datetime
import re
from collections import defaultdict

# åª’ä½“æƒå¨åº¦æ˜ å°„å­—å…¸
media_authority_ranking = {
    'æœ€é«˜æƒå¨åª’ä½“': [
        'å›½å®¶å¸‚åœºç›‘ç£ç®¡ç†æ€»å±€',  # æ±½è½¦å¬å›å®˜æ–¹å‘å¸ƒæœºæ„
        'å›½åŠ¡é™¢å‘å±•ç ”ç©¶ä¸­å¿ƒä¿¡æ¯ç½‘',
        'ç¦å»ºçœå¸‚åœºç›‘ç£ç®¡ç†å±€ï¼ˆçŸ¥è¯†äº§æƒå±€ï¼‰',
        'æµ·å—çœå¸‚åœºç›‘ç£ç®¡ç†å±€',
        'é€šåŒ–å¸‚äººæ°‘æ”¿åºœ',
        'èŠœæ¹–å¸‚äººæ°‘æ”¿åºœ',

        # å›½å®¶çº§ä¸»æµåª’ä½“
        'äººæ°‘æ—¥æŠ¥ APP',
        'æ–°åç½‘',
        'äººæ°‘ç½‘',
        'å¤®è§†ç½‘',
        'ä¸­å›½ç½‘',
        'å…‰æ˜ç½‘',
        'ä¸­å›½é’å¹´ç½‘',
        'ä¸­é’åœ¨çº¿',
        'ä¸­å·¥ç½‘',
        'ä¸­å›½æ–°é—»ç¤¾',
        'å‚è€ƒæ¶ˆæ¯ç½‘',

        # ä¸“ä¸šè´¨é‡ç›‘ç®¡
        'ä¸­å›½è´¨é‡æ–°é—»ç½‘',
        'ä¸­å›½è´¨é‡æŠ¥(æ•°å­—æŠ¥)'
    ],  # ç¬¬ä¸€å±‚çº§
    'é«˜æƒå¨åª’ä½“': [
        # ä¸“ä¸šè´¢ç»åª’ä½“
        'è´¢è”ç¤¾',
        'ç¬¬ä¸€è´¢ç» APP',
        'è´¢ç»ç½‘',
        'æ¾æ¹ƒæ–°é—» APP',
        'ç•Œé¢æ–°é—» APP',
        'æ¯æ—¥ç»æµæ–°é—» APP',
        'ç»æµè§‚å¯Ÿç½‘',
        'ä¸­å›½ç»è¥æŠ¥',
        'ç»æµå‚è€ƒæŠ¥',
        'åå¤æ—¶æŠ¥',
        'æŠ•èµ„è€…ç½‘',

        # è¯åˆ¸æƒå¨åª’ä½“
        'ä¸Šæµ·è¯åˆ¸æŠ¥ APP',
        'è¯åˆ¸æ—¶æŠ¥ APP',
        'è¯åˆ¸æ—¶æŠ¥ç½‘',
        'ä¸­å›½è¯åˆ¸ç½‘',
        'ä¸­å›½åŸºé‡‘æŠ¥',
        'æœŸè´§æ—¥æŠ¥',
        'ä¸­å›½é“¶è¡Œä¿é™©æŠ¥',

        # ä¸“ä¸šæ±½è½¦åª’ä½“
        'ä¸­å›½æ±½è½¦å¬å›ç½‘',  # ä¸“ä¸šå¬å›ä¿¡æ¯ç½‘ç«™
        'è½¦è´¨ç½‘',  # ä¸“ä¸šæ±½è½¦è´¨é‡å¹³å°
        'ç›–ä¸–æ±½è½¦ç½‘',
        'ä¸­å›½æ±½è½¦æŠ¥ç½‘'
    ],  # ç¬¬äºŒå±‚çº§
    'ä¸­ç­‰æƒå¨åª’ä½“': [
        # ä¸»æµç»¼åˆæ–°é—»
        'è…¾è®¯æ–°é—» APP',
        'ç½‘æ˜“æ–°é—» APP',
        'æ–°æµªæ–°é—» APP',
        'ä»Šæ—¥å¤´æ¡',
        'ç™¾åº¦ APP',
        'å‡¤å‡°æ–°é—» APP',
        'ç¯çƒç½‘',

        # ä¸“ä¸šèµ„è®¯å¹³å°
        '36æ°ª',
        'è™å—…ç½‘ APP',
        'é’›åª’ä½“ç½‘',
        'åˆ›ä¸šé‚¦ APP',
        'å“ç© APP',
        'äº¿æ¬§ç½‘',
        'æ™ºé€šè´¢ç»ç½‘',
        'æ™ºé€šè´¢ç» APP',
        'ç§‘åˆ›æ¿æ—¥æŠ¥',

        # å‚ç›´æ±½è½¦åª’ä½“
        'æ‡‚è½¦å¸ APP',
        'æ±½è½¦ä¹‹å®¶',
        'æ˜“è½¦ç½‘',
        'å¤ªå¹³æ´‹æ±½è½¦ç½‘',
        'çˆ±å¡æ±½è½¦ç½‘',
        'æ–°æµªæ±½è½¦',
        'æœç‹æ±½è½¦',
        'ç½‘ä¸Šè½¦å¸‚ APP',
        'æ±½è½¦å¤´æ¡ APP'
    ],  # ç¬¬ä¸‰å±‚çº§
    'åŸºç¡€æƒå¨åª’ä½“': [
        # è´¢ç»æ•°æ®æœåŠ¡
        'ä¸œæ–¹è´¢å¯Œç½‘',
        'ä¸œæ–¹è´¢å¯Œ APP',
        'åŒèŠ±é¡ºè´¢ç»',
        'åŒèŠ±é¡º APP',
        'åŒèŠ±é¡ºiFinD APP',
        'é›ªçƒ',
        'é‡‘èç•Œ',
        'å’Œè®¯ç½‘',
        'è¯åˆ¸ä¹‹æ˜Ÿ',
        'åå°”è¡—è§é—»',
        'ä¸­æ–°ç»çº¬ APP',
        'æ¯ç»ç½‘',

        # åœ°æ–¹ä¸»æµåª’ä½“
        'åŒ—äº¬æ—¥æŠ¥ APP',
        'æ¹–åŒ—æ—¥æŠ¥ APP',
        'æµ·å—æ—¥æŠ¥ APP',
        'å—æ–¹è´¢ç»ç½‘',
        'çº¢ç½‘',
        'äº‘å—ç½‘',
        'å¤§ä¼—ç½‘',
        'æ²³åŒ—æ–°é—»ç½‘',
        'è†æ¥šç½‘-æ¹–åŒ—æ—¥æŠ¥ç½‘',
        'å±±è¥¿æ™šæŠ¥',
        'åˆè‚¥åœ¨çº¿',
        'å®‰å¾½ç½‘',
        'æ²³å—ä¸€ç™¾åº¦',

        # åˆ¸å•†ç ”ç©¶
        'ä¸­ä¿¡å»ºæŠ•è¯åˆ¸ APP',
        'å›½æ³°å›å®‰è¯åˆ¸ APP',
        'æµ·é€šeæµ·é€šè´¢ APP'
    ],  # ç¬¬å››å±‚çº§
    'ä¸€èˆ¬æƒå¨åª’ä½“': [
        # è´¢ç»åª’ä½“
        'è´¢ç»å¤´æ¡ APP',
        'æ–°æµªè´¢ç» APP',
        'é‡‘æŠ•ç½‘',
        'æ±‡é€šè´¢ç»',
        'æ ¼éš†æ±‡',
        'å¯Œé€”ç‰›ç‰›',
        'å¤§æ™ºæ…§ APP',
        'ä¸­å›½è´¢å¯Œ APP',
        'æŒè¯å®å¤©ç‘ç‰ˆ APP',
        'è…¾è®¯è‡ªé€‰è‚¡ APP',
        'BBAEå¿…è´è¯åˆ¸ APP',
        'æ™ºè¿œå£¹æˆ·é€š APP',
        'å¤©å¤©åŸºé‡‘ç½‘ APP',
        'è§é—»VIP APP',
        'è‹±ä¸ºè´¢æƒ…',
        'æ±‡é‡‘ç½‘',
        'å…¨æ™¯ç½‘',
        'ä¸­è´¢ç½‘',

        # è¡Œä¸šå‚ç›´
        'æˆ‘çš„é’¢é“ç½‘',
        'é•¿æ±Ÿæœ‰è‰²é‡‘å±ç½‘',
        'ä¸­å›½é’¢é“å·¥ä¸šä¿¡æ¯ç½‘',
        'ä¸­å›½äº§ä¸šç»æµä¿¡æ¯ç½‘',

        # ä¸“ä¸šèµ„è®¯
        'å•†ä¸šæ–°çŸ¥',
        'DoNews',
        'æœªå¤®ç½‘',
        'æ”¯ä»˜ç™¾ç§‘ç½‘',
        'ä¼ä¸šæ—¶æŠ¥ç½‘'
    ],  # ç¬¬äº”å±‚çº§
    'è¾ƒä½æƒå¨åª’ä½“': [
        # å†…å®¹èšåˆå¹³å°
        'ä¸€ç‚¹èµ„è®¯',
        'ä¸€ç‚¹èµ„è®¯ APP',
        'UCå¤´æ¡ APP',
        'ZAKER APP',
        'è¶£å¤´æ¡ APP',
        'çœ‹ç‚¹å¿«æŠ¥ APP',
        'å¿«èµ„è®¯',
        'äº‘æŒè´¢ç»',
        'ä¸­é‡‘åœ¨çº¿',
        'hao123æ–°é—»',
        'æ‰‹æœºæ–°æµªç½‘',
        'æ‰‹æœºæœç‹ APP',
        'æ‰‹æœºç½‘æ˜“ç½‘',

        # ä¸“ä¸šç¤¾äº¤å¹³å°
        'çŸ¥ä¹',
        'çŸ¥ä¹ä¸“æ ',
        'é›ªçƒ',  # å…¼å…·ç¤¾äº¤å±æ€§

        # æ±½è½¦èµ„è®¯
        'è½¦è®¯ç½‘',
        'æ±½è½¦æŠ•è¯‰ç½‘',
        'ç”µè½¦ä¹‹å®¶',
        'ç¬¬1ç”µåŠ¨',
        '315æ±½è½¦ç½‘',
        'æ±½è½¦æ–°çœ‹ç‚¹',
        'ä¸­å›½æ±½è½¦å¤´æ¡'
    ],  # ç¬¬å…­å±‚çº§
    'æœ€ä½æƒå¨åª’ä½“': [
        # ç¤¾äº¤å¨±ä¹å¹³å°
        'æ–°æµªå¾®åš',
        'æŠ–éŸ³ APP',
        'å¿«æ‰‹ APP',
        'å“”å“©å“”å“©APP',
        'å°çº¢ä¹¦ APP',
        'è™æ‰‘ç¤¾åŒº',

        # è®ºå›ç¤¾åŒº
        'æ–°æµªè‚¡å¸‚æ±‡',
        'åŒèŠ±é¡ºåœˆå­è®ºå›',
        'ä¸œæ–¹è´¢å¯Œç½‘è‚¡å§',
        'å¤ªå¹³æ´‹æ±½è½¦ç½‘è®ºå›',
        'æ±½è½¦ä¹‹å®¶è®ºå›',
        'è‚¡å§ APP',
        'æ€å¦',

        # è‡ªåª’ä½“ä¸å°å‹ç½‘ç«™
        'å¾®ä¿¡å…¬ä¼—å·',
        'é‡‘èå…«å¦å¥³ APP',
        'è´¢ç»é’»',
        'æ€ç»´è´¢ç»',
        'é¡¶å°–è´¢ç»ç½‘',
        'ç™¾å®¶è´¢å¯Œç½‘',
        'ä¸­å›½è´¢è®¯ç½‘',
        'æ½®èµ·ç½‘',
        'è´¢ç»äº§ä¸šç½‘',
        'æ–‡è´¢ç½‘',
        'ä¼˜è´¢ç½‘',
        'å‘ç°ç½‘',
        'æˆåŠŸè´¢ç»ç½‘',
        'åšè§ˆé»„é¡µ',
        'æœ€èµ„è®¯',
        'ä¸­æŠ•ç½‘',

        # å…¶ä»–
        'èŠ±èŠ±å¥³æ€§ç½‘',
        'Bianews',
        'xw.qq.com',
        'å®œæ˜¥æ–°é—»ç½‘',
        'é’¢ä¼ç½‘',
        'äº‘èšç½‘',
        'ä¸­æœæœæ‚¦ APP',
        'ä¸­é’çœ‹ç‚¹APP',
        'ç¯çƒTIME APP',
        'å¤®å¹¿ç½‘ APP',
        'å°é¢',
        'å‘¨åˆ°ä¸Šæµ·ç½‘',
        'ä¹å±…è´¢ç» APP',
        'å¸‚å€¼é£äº‘',
        'é¢åŒ…è´¢ç»',
        'èŠ‚ç‚¹è´¢ç»',
        'é²¸å‡†',
        'çƒ¯ç‰›æ•°æ®',
        'ä¼æŸ¥æŸ¥',
        'å¤©çœ¼æŸ¥',
        'å¯ä¿¡å®',
        'çˆ±ä¼æŸ¥',
        'ä¼ä¿¡å®'
    ]  # ç¬¬ä¸ƒå±‚çº§
}

# æ„å»ºåª’ä½“åˆ°æƒå¨åº¦åˆ†æ•°çš„æ˜ å°„å­—å…¸
media_authority_map = {}
authority_scores = {
    'æœ€é«˜æƒå¨åª’ä½“': 1,
    'é«˜æƒå¨åª’ä½“': 2,
    'ä¸­ç­‰æƒå¨åª’ä½“': 3,
    'åŸºç¡€æƒå¨åª’ä½“': 4,
    'ä¸€èˆ¬æƒå¨åª’ä½“': 5,
    'è¾ƒä½æƒå¨åª’ä½“': 6,
    'æœ€ä½æƒå¨åª’ä½“': 7
}

for level_name, media_list in media_authority_ranking.items():
    score = authority_scores[level_name]
    for media in media_list:
        media_authority_map[media] = score


def get_media_authority(media_name):
    """è·å–åª’ä½“æƒå¨åº¦åˆ†æ•°ï¼ˆæ•°å€¼è¶Šå°æƒå¨åº¦è¶Šé«˜ï¼‰"""
    return media_authority_map.get(media_name, 7)  # é»˜è®¤æœ€ä½æƒå¨åº¦


class AdvancedNewsDeduplicator:
    def __init__(self, simhash_threshold=3, min_content_length=50):
        """
        åˆå§‹åŒ–é«˜çº§å»é‡å™¨

        Args:
            simhash_threshold: SimHashæ±‰æ˜è·ç¦»é˜ˆå€¼
            min_content_length: æœ€å°å†…å®¹é•¿åº¦ï¼ŒçŸ­äºæ­¤é•¿åº¦çš„ä¸å‚ä¸å»é‡
        """
        self.simhash_threshold = simhash_threshold
        self.min_content_length = min_content_length

    def preprocess_text(self, text):
        """æ–‡æœ¬é¢„å¤„ç†"""
        if pd.isna(text):
            return ""
        # å»é™¤HTMLæ ‡ç­¾ã€ç‰¹æ®Šå­—ç¬¦ç­‰
        text = re.sub(r'<[^>]+>', '', str(text))
        text = re.sub(r'[^\w\u4e00-\u9fff]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def calculate_simhash(self, text):
        """è®¡ç®—æ–‡æœ¬çš„SimHashå€¼"""
        text = self.preprocess_text(text)
        if len(text) < self.min_content_length:
            return None  # å†…å®¹å¤ªçŸ­ï¼Œä¸å‚ä¸å»é‡
        return Simhash(text).value

    def hamming_distance(self, hash1, hash2):
        """è®¡ç®—ä¸¤ä¸ªSimHashçš„æ±‰æ˜è·ç¦»"""
        if hash1 is None or hash2 is None:
            return float('inf')  # å†…å®¹å¤ªçŸ­çš„è¿”å›æ— é™å¤§è·ç¦»
        return bin(hash1 ^ hash2).count('1')

    def find_duplicate_groups(self, df, content_col='å†…å®¹'):
        print("å¼€å§‹è®¡ç®—SimHash...")
        df = df.copy()
        df['simhash'] = df[content_col].apply(self.calculate_simhash)

        print("æ„å»ºSimHashç´¢å¼•...")
        hash_to_indices = defaultdict(list)
        for idx, simhash in zip(df.index, df['simhash']):
            # è¿‡æ»¤æ‰ NaN å’Œ None
            if pd.isna(simhash):
                continue
            # ç¡®ä¿æ˜¯ int
            simhash = int(simhash)
            hash_to_indices[simhash].append(idx)

        print("å¯»æ‰¾ç›¸ä¼¼æ–°é—»ç»„...")
        duplicate_groups = []
        processed_hashes = set()

        for base_hash, base_indices in hash_to_indices.items():
            if base_hash in processed_hashes:
                continue

            current_group = set(base_indices)

            for other_hash, other_indices in hash_to_indices.items():
                if other_hash in processed_hashes:
                    continue
                if other_hash == base_hash:
                    continue

                if self.hamming_distance(base_hash, other_hash) <= self.simhash_threshold:
                    current_group.update(other_indices)
                    processed_hashes.add(other_hash)

            if len(current_group) > 1:
                duplicate_groups.append(list(current_group))

            processed_hashes.add(base_hash)

        print(f"æ‰¾åˆ° {len(duplicate_groups)} ç»„é‡å¤æ–°é—»")
        return duplicate_groups

    def select_best_article(self, df, duplicate_indices, date_col='æ—¥æœŸ', word_count_col='å­—æ•°'):
        """
        ä»é‡å¤æ–°é—»ç»„ä¸­é€‰æ‹©æœ€ä½³æ–‡ç« 

        é€‰æ‹©ç­–ç•¥ï¼ˆä¼˜å…ˆçº§é¡ºåºï¼‰ï¼š
        1. åª’ä½“æƒå¨åº¦ï¼ˆæ•°å€¼è¶Šå°è¶Šå¥½ï¼‰
        2. å‘å¸ƒæ—¶é—´ï¼ˆè¶Šæ—©è¶Šå¥½ï¼‰
        3. æ–‡ç« å­—æ•°ï¼ˆè¶Šå¤šè¶Šå¥½ï¼‰
        4. åª’ä½“ç±»å‹ä¼˜å…ˆçº§ï¼ˆå®˜æ–¹æœºæ„ > ä¸“ä¸šåª’ä½“ > ç»¼åˆåª’ä½“ > ç¤¾äº¤å¹³å°ï¼‰
        """
        if len(duplicate_indices) == 0:
            return None

        articles = df.loc[duplicate_indices].copy()

        # è®¡ç®—æƒå¨åº¦åˆ†æ•°
        articles['authority_score'] = articles['åª’ä½“åç§°'].apply(get_media_authority)

        # åª’ä½“ç±»å‹ä¼˜å…ˆçº§æ˜ å°„
        media_type_priority = {
            'å®˜æ–¹æœºæ„': 1,
            'è´¢ç»åª’ä½“': 2,
            'å‚ç›´æ±½è½¦': 3,
            'ä¸“ä¸šèµ„è®¯': 4,
            'ä¼ ç»Ÿåª’ä½“è½¬å‹': 5,
            'ç»¼åˆæ–°é—»': 6,
            'è¡Œä¸šå‚ç›´': 7,
            'åœ°æ–¹åª’ä½“': 8,
            'åˆ¸å•†å¹³å°': 9,
            'å†…å®¹èšåˆ': 10,
            'ç¤¾äº¤å¹³å°': 11,
            'å…¶ä»–': 12
        }

        articles['type_priority'] = articles.get('åª’ä½“ç±»å‹', 'å…¶ä»–').apply(
            lambda x: media_type_priority.get(x, 12)
        )

        # å¤šé‡æ’åºï¼šæƒå¨åº¦ â†’ æ—¥æœŸ â†’ å­—æ•° â†’ åª’ä½“ç±»å‹
        articles_sorted = articles.sort_values(
            by=['authority_score', date_col, word_count_col, 'type_priority'],
            ascending=[True, True, False, True]
        )

        best_article_idx = articles_sorted.index[0]

        # è®°å½•é€‰æ‹©åŸå› 
        best_article = articles_sorted.iloc[0]
        selection_reason = self._get_selection_reason(best_article, articles_sorted)

        return best_article_idx, selection_reason

    def _get_selection_reason(self, best_article, all_articles):
        """ç”Ÿæˆé€‰æ‹©åŸå› è¯´æ˜"""
        reasons = []

        # æ£€æŸ¥æƒå¨åº¦æ˜¯å¦æœ€ä¼˜
        min_authority = all_articles['authority_score'].min()
        if best_article['authority_score'] == min_authority:
            reasons.append(f"æƒå¨åº¦æœ€é«˜({best_article['authority_score']})")

        # æ£€æŸ¥æ˜¯å¦æœ€æ—©å‘å¸ƒ
        min_date = all_articles['æ—¥æœŸ'].min()
        if best_article['æ—¥æœŸ'] == min_date:
            reasons.append("å‘å¸ƒæ—¶é—´æœ€æ—©")

        # æ£€æŸ¥å­—æ•°æ˜¯å¦æœ€å¤š
        max_words = all_articles['å­—æ•°'].max()
        if best_article['å­—æ•°'] == max_words:
            reasons.append("å†…å®¹æœ€å®Œæ•´")

        return " + ".join(reasons) if reasons else "é»˜è®¤é€‰æ‹©"

    def deduplicate_news(self, df, content_col='å†…å®¹', date_col='æ—¥æœŸ', word_count_col='å­—æ•°'):
        """
        ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„æ–°é—»å»é‡æµç¨‹
        """
        print("å¼€å§‹æ–°é—»å»é‡å¤„ç†...")
        print(f"åŸå§‹æ•°æ®é‡: {len(df)} æ¡")

        # æ•°æ®é¢„å¤„ç†
        df = df.copy()
        if df[date_col].dtype != 'datetime64[ns]':
            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')

        # æ‰¾å‡ºé‡å¤ç»„
        duplicate_groups = self.find_duplicate_groups(df, content_col)

        # å¤„ç†é‡å¤ç»„
        to_remove_indices = set()
        selection_log = []
        stats = {
            'total_groups': len(duplicate_groups),
            'authority_distribution': defaultdict(int),
            'removed_count': 0
        }

        for group_id, group_indices in enumerate(duplicate_groups):
            best_idx, reason = self.select_best_article(df, group_indices, date_col, word_count_col)

            if best_idx is not None:
                best_article = df.loc[best_idx]
                authority_level = get_media_authority(best_article['åª’ä½“åç§°'])

                # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                stats['authority_distribution'][authority_level] += 1

                # è®°å½•é€‰æ‹©æ—¥å¿—
                selection_log.append({
                    'group_id': group_id,
                    'best_index': best_idx,
                    'best_media': best_article['åª’ä½“åç§°'],
                    'authority_level': authority_level,
                    'selection_reason': reason,
                    'duplicate_count': len(group_indices),
                    'removed_count': len(group_indices) - 1
                })

                # æ ‡è®°åˆ é™¤
                for idx in group_indices:
                    if idx != best_idx:
                        to_remove_indices.add(idx)

        stats['removed_count'] = len(to_remove_indices)

        # åˆ›å»ºå»é‡åçš„DataFrame
        deduplicated_df = df[~df.index.isin(to_remove_indices)].copy()

        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        self._generate_detailed_report(df, deduplicated_df, selection_log, stats)

        return deduplicated_df, selection_log

    def _generate_detailed_report(self, original_df, deduplicated_df, selection_log, stats):
        """ç”Ÿæˆè¯¦ç»†çš„å»é‡æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("æ–°é—»å»é‡è¯¦ç»†æŠ¥å‘Š")
        print("=" * 60)

        # åŸºç¡€ç»Ÿè®¡
        print(f"\nğŸ“Š åŸºç¡€ç»Ÿè®¡:")
        print(f"  åŸå§‹æ•°æ®é‡: {len(original_df):,} æ¡")
        print(f"  å»é‡åæ•°æ®é‡: {len(deduplicated_df):,} æ¡")
        print(f"  åˆ é™¤é‡å¤æ–°é—»: {stats['removed_count']:,} æ¡")
        print(f"  é‡å¤æ–°é—»ç»„æ•°: {stats['total_groups']:,} ç»„")
        print(f"  å»é‡ç‡: {stats['removed_count'] / len(original_df) * 100:.2f}%")

        # æƒå¨åº¦åˆ†å¸ƒ
        print(f"\nğŸ… æƒå¨åº¦åˆ†å¸ƒ (è¢«ä¿ç•™çš„æ–°é—»ç»„):")
        authority_names = {
            1: "æœ€é«˜æƒå¨", 2: "é«˜æƒå¨", 3: "ä¸­ç­‰æƒå¨",
            4: "åŸºç¡€æƒå¨", 5: "ä¸€èˆ¬æƒå¨", 6: "è¾ƒä½æƒå¨", 7: "æœ€ä½æƒå¨"
        }

        for auth_level in sorted(stats['authority_distribution'].keys()):
            count = stats['authority_distribution'][auth_level]
            percentage = count / stats['total_groups'] * 100 if stats['total_groups'] > 0 else 0
            print(f"  {authority_names[auth_level]}({auth_level}): {count} ç»„ ({percentage:.1f}%)")

        # é€‰æ‹©åŸå› ç»Ÿè®¡
        print(f"\nğŸ¯ é€‰æ‹©åŸå› ç»Ÿè®¡:")
        reason_stats = defaultdict(int)
        for log in selection_log:
            reason_stats[log['selection_reason']] += 1

        for reason, count in sorted(reason_stats.items(), key=lambda x: x[1], reverse=True):
            percentage = count / len(selection_log) * 100 if selection_log else 0
            print(f"  {reason}: {count} ç»„ ({percentage:.1f}%)")

        # æ˜¾ç¤ºå‰å‡ ä¸ªå¤„ç†ç¤ºä¾‹
        if selection_log:
            print(f"\nğŸ“‹ å‰10ç»„å¤„ç†ç¤ºä¾‹:")
            for i, log in enumerate(selection_log[:10]):
                print(f"  ç»„{log['group_id'] + 1}: ä¿ç•™ '{log['best_media']}' "
                      f"(æƒå¨åº¦{log['authority_level']}), åŸå› : {log['selection_reason']}, "
                      f"åˆ é™¤ {log['removed_count']} ä¸ªé‡å¤")


# æ‰¹é‡å¤„ç†å‡½æ•°
def process_large_dataset(df, batch_size=10000):
    """
    å¤„ç†å¤§å‹æ•°æ®é›†çš„å‡½æ•°ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
    """
    print(f"å¼€å§‹åˆ†æ‰¹å¤„ç†æ•°æ®é›†ï¼Œæ¯æ‰¹ {batch_size} æ¡...")

    deduplicator = AdvancedNewsDeduplicator(simhash_threshold=3)
    all_deduplicated = []
    all_selection_logs = []

    # æŒ‰æ—¶é—´åˆ†æ‰¹ï¼ˆå‡è®¾æ•°æ®æŒ‰æ—¥æœŸæ’åºï¼‰
    if 'å¹´æœˆ' in df.columns:
        unique_months = df['å¹´æœˆ'].unique()
        print(f"æŒ‰æœˆä»½åˆ†æ‰¹å¤„ç†: {len(unique_months)} ä¸ªæœˆ")

        for month in sorted(unique_months):
            monthly_data = df[df['å¹´æœˆ'] == month].copy()
            print(f"å¤„ç† {month}: {len(monthly_data)} æ¡æ•°æ®")

            if len(monthly_data) > 0:
                deduplicated_month, logs = deduplicator.deduplicate_news(monthly_data)
                all_deduplicated.append(deduplicated_month)
                all_selection_logs.extend(logs)
    else:
        # ç®€å•åˆ†æ‰¹
        total_batches = (len(df) + batch_size - 1) // batch_size
        for i in range(total_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, len(df))
            batch_data = df.iloc[start_idx:end_idx].copy()

            print(f"å¤„ç†æ‰¹æ¬¡ {i + 1}/{total_batches}: {len(batch_data)} æ¡æ•°æ®")
            deduplicated_batch, logs = deduplicator.deduplicate_news(batch_data)
            all_deduplicated.append(deduplicated_batch)
            all_selection_logs.extend(logs)

    # åˆå¹¶ç»“æœ
    final_deduplicated = pd.concat(all_deduplicated, ignore_index=True)

    print(f"\næ‰¹é‡å¤„ç†å®Œæˆ!")
    print(f"æœ€ç»ˆæ•°æ®é‡: {len(final_deduplicated):,} æ¡")
    print(f"æ€»åˆ é™¤æ•°é‡: {len(df) - len(final_deduplicated):,} æ¡")

    return final_deduplicated, all_selection_logs


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    print("é«˜çº§æ–°é—»å»é‡å™¨åˆå§‹åŒ–å®Œæˆ!")
    print(f"åª’ä½“æƒå¨åº¦åˆ†çº§: å…±{len(media_authority_map)}ä¸ªåª’ä½“")

    # ç»Ÿè®¡å„æƒå¨åº¦åª’ä½“æ•°é‡
    authority_counts = defaultdict(int)
    for media, score in media_authority_map.items():
        authority_counts[score] += 1

    print("\nåª’ä½“æƒå¨åº¦åˆ†å¸ƒ:")
    for score in sorted(authority_counts.keys()):
        print(f"  æƒå¨åº¦{score}: {authority_counts[score]}ä¸ªåª’ä½“")


df = pd.read_excel("cleaned_normalized_data_full.xlsx")
deduplicator = AdvancedNewsDeduplicator(simhash_threshold=3)
result_df, selection_logs = deduplicator.deduplicate_news(df)

result_df.to_excel("deduplicated_news_full.xlsx", index=False)
print("å·²æˆåŠŸä¿å­˜ä¸º deduplicated_news_full.xlsx")
